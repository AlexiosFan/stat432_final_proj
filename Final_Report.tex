\documentclass{article}
\usepackage{amssymb}
\usepackage{babel}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{framed}
\usepackage{pifont}
\usepackage{listings}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{float}

\usetikzlibrary{automata, positioning, arrows}


\def\ra{\rightarrow}
\def\rr{\Rightarrow}
\def\tf{$\Rightarrow$}
\def\oo{\infty}
\def\l/{\backslash}
\def\0{\emptyset}
\def\P{\mathbb{P}}
\def\px{\mathcal{P}_X}
\def\s{\mathcal{S}}
\def\a{\mathcal{A}}
\def\bs{\mathcal{B}}
\def\lm{\mathcal{L}}
\def\R{\mathbb{R}}
\def\E{\mathbb{E}}
\def\Z{\mathbb{Z}}
\def\m{\mathcal{M}}
\def\N{\mathcal{N}}
\def\b{\,\,\,}




\title{STAT 432 Final Project}
\author{Ruiming Min(rmin4), Zixuan Fan(zixuanf5), Yewen Li(yewenli2)}
\date{\today}

\begin{document}

\maketitle

\section{Project Description and Summary}

\section{Literature Review}
On the Kaggle website, there are many discussions with profound quality and insightful materials. 
In general, participants tend to use the modern deep learning models such as convolution neural network,
transformer model etc. The classical statistical learning models do not prove to be effective in this competition. 
Potential reasons for this phenomenon are that the classical models are not able to capture the complex patterns in the data. 
The writing style of the text does not follow any explicit linear or quadratic patterns. 
For this reason, the regression models, which is based on the linear assumption, are not able to capture the patterns in the data.
On the other hand, the classification models, also makes assumptions on the distribution of data in certain clusters, 
while the distribution of the handwriting data is highly dependent on our data processing. 
Thus, the classification models may not be effective, either, because some patterns are either lost in processing or cannot be summarized into single 
record for each observation. 

Looking into various discussions, we found two revealed methods highly effective with detailed descriptions. 
We analyze those methods and exploit some features in our project. 
\subsection{LGBM Regression}
\href{https://www.kaggle.com/code/hengzheng/link-writing-simple-lgbm-baseline}{Link Writing Simple LGBM baseline} by ZhengHeng
presented the author's Python implementation of LGBM regression model for score prediction. The author's approach 
reached a score of 0.589 and ranked 382nd on the leaderboard. Although the author was not amongst the top of the leaderboard, 
his approach was simple and inspired many other people, as shown in this \href{https://www.kaggle.com/competitions/linking-writing-processes-to-writing-quality/discussion/451081}{discussion}.

ZhengHeng's approach can be summarized into two steps. First, he did a feature engineering on the data. 
By considering the physical meaning of the relationship between features, he generated some new features from 
the existing log and discarded those high correlated features. Some of new features are the ratio of 
\textbf{word count}, \textbf{idle time} and \textbf{event time}. Second, he trained the LGBM regression model
on a five-fold cross validation. The Light Gradient Boosting Machine (LGBM) is a gradient boosting framework
that uses tree based learning algorithms. 
This newly developed model grows tree vertically in a leaf-wise fashion, while the traditional tree-based models
grow horizontally in a level-wise fashion. The author tuned the model manually and publish the best parameters. 
% Decide if we want to use this regressor?


\section{Data processing}


\section{Unsupervised Learning}

\subsection{* model1}


\subsection{* model2}


\section{Regression/Classification Models}

\subsection{Regression Model 1: *}

\subsection{Regression Model 2: *}

\subsection{Regression Model 3: *}

\subsection{Classification Model: Random Forest}
For classification, we choose the random forest model using the R package \texttt{randomForest}.
The simple fitting using the default parameters gives us an \texttt{accuracy} of $\alpha = 0.2848$. 
Taking look at the confusion matrix, we found that prediction is concentrated along the diagonal, 
but does not have a high accuracy. So we present a new metric, \texttt{accuracy with tolerance}. 
This is the percentage of observations that are predicted within a tolerance $0.5$ of the true value.
\begin{align*}
    \tau = \texttt{Accuracy with tolerance} = \dfrac{\sum \texttt{diag} + \sum \texttt{subdiag} + \sum \texttt{superdiag}}{\sum \texttt{Observations}}
\end{align*}
In the simple fitting, $\tau = 0.6909$, which proves a good concentrtion of random forest model. 
\paragraph{Parameter Tuning}
The tuning goal of the model is to increase both the \texttt{accuracy} and the \texttt{accuracy with tolerance}.
Amongst many tuning parameters of random forest, we focus on \texttt{mtry} and \texttt{nodesize} with default 
\texttt{ntree} = 500. The grid we used is 
\begin{align*}
    \texttt{mtry} &\in \{1, 3, 5, 8, 10, 12, 15, 20, 25, 40,  50 \} \\
    \texttt{nodesize} &\in \{1, 3, 5, 8, 10, 12, 15, 20, 25, 40,  50 \}
\end{align*}
where the default values are $\texttt{mtry} = 3$ and $\texttt{nodesize} = 1$. 
The \hyperref[fig:contour]{contour plots} shows a best fit at $\texttt{mtry} \approx 3$ and $\texttt{nodesize} \approx 10$.
We may conclude that the effect of the random forest model is limited at $\alpha \approx 0.3$
and $\tau \approx 0.7$. Since there is an obvious concentration, 
this model is useful in the sense that it can show a general trend of the score.
However, it is not a good choice for sophisticated prediction as of the low accuracy.
\begin{figure}[H]
    \includegraphics*[scale=0.25]{figures/contour_plot_accuracy.png}
    \includegraphics*[scale=0.25]{figures/contour_plot_accuracy_with_tolerance.png}
    \caption{Contour plots of \texttt{accuracy} and \texttt{accuracy with tolerance}}
    \label{fig:contour}
\end{figure}

\paragraph{Variable Importance}
Another property of the random forest model is that it can provide a report on variable importance. 
Taking a look at \texttt{MeanDecreaseAccuracy} and \texttt{MeanDecreaseGini}, 
we found that the ratios features are much more importance than the raw value features. 
For instance, the \texttt{MeanDecreaseAccuracy} of \texttt{ratio\_word\_count} is $81.40$, 
while that of \texttt{Input\_count} is only $5.94$. We may tell from this phenomenon that
the ratio features are more informative than the raw value features. 
And a good feature engineering strategy results in better classification accuracy. 

\end{document}
\documentclass{article}
\usepackage{amssymb}
\usepackage{babel}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{framed}
\usepackage{pifont}
\usepackage{listings}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{float}

\usetikzlibrary{automata, positioning, arrows}


\def\ra{\rightarrow}
\def\rr{\Rightarrow}
\def\tf{$\Rightarrow$}
\def\oo{\infty}
\def\l/{\backslash}
\def\0{\emptyset}
\def\P{\mathbb{P}}
\def\px{\mathcal{P}_X}
\def\s{\mathcal{S}}
\def\a{\mathcal{A}}
\def\bs{\mathcal{B}}
\def\lm{\mathcal{L}}
\def\R{\mathbb{R}}
\def\E{\mathbb{E}}
\def\Z{\mathbb{Z}}
\def\m{\mathcal{M}}
\def\N{\mathcal{N}}
\def\b{\,\,\,}




\title{STAT 432 Final Project}
\author{Ruiming Min(rmin4), Zixuan Fan(zixuanf5), Yewen Li(yewenli2)}
\date{\today}

\begin{document}

\maketitle

\section{Project Description and Summary}

\section{Literature Review}
On the Kaggle website, there are many discussions with profound quality and insightful materials. 
In general, participants tend to use the modern deep learning models such as convolution neural network,
transformer model etc. The classical statistical learning models do not prove to be effective in this competition. 
Potential reasons for this phenomenon are that the classical models are not able to capture the complex patterns in the data. 
The writing style of the text does not follow any explicit linear or quadratic patterns. 
For this reason, the regression models, which is based on the linear assumption, are not able to capture the patterns in the data.
On the other hand, the classification models, also makes assumptions on the distribution of data in certain clusters, 
while the distribution of the handwriting data is highly dependent on our data processing. 
Thus, the classification models may not be effective, either, because some patterns are either lost in processing or cannot be summarized into single 
record for each observation. 

Looking into various discussions, we found two revealed methods highly effective with detailed descriptions. 
We analyze those methods and exploit some features in our project. 
\subsection{LGBM Regression}
\href{https://www.kaggle.com/code/hengzheng/link-writing-simple-lgbm-baseline}{Link Writing Simple LGBM baseline} by ZhengHeng
presented the author's Python implementation of LGBM regression model for score prediction. The author's approach 
reached a score of 0.589 and ranked 382nd on the leaderboard. Although the author was not amongst the top of the leaderboard, 
his approach was simple and inspired many other people, as shown in this \href{https://www.kaggle.com/competitions/linking-writing-processes-to-writing-quality/discussion/451081}{discussion}.

ZhengHeng's approach can be summarized into two steps. First, he did a feature engineering on the data. 
By considering the physical meaning of the relationship between features, he generated some new features from 
the existing log and discarded those high correlated features. Some of new features are the ratio of 
\textbf{word count}, \textbf{idle time} and \textbf{event time}. Second, he trained the LGBM regression model
on a five-fold cross validation. The Light Gradient Boosting Machine (LGBM) is a gradient boosting framework
that uses tree based learning algorithms. 
This newly developed model grows tree vertically in a leaf-wise fashion, while the traditional tree-based models
grow horizontally in a level-wise fashion. The author tuned the model manually and publish the best parameters. 
% Decide if we want to use this regressor?


\section{Data processing}


\section{Unsupervised Learning}

\subsection{* model1}


\subsection{* model2}


\section{Regression/Classification Models}

\subsection{Regression Model 1: Linear Regression with ridge/lasso penalty}
Since we have learnt from the Kaggle discussion that the linear regression models are not really effective for this dataset, 
we consider a penalty and see if it leads to a good performance. 
We start with raw Ridge and LASSO regression models. Using the \texttt{glmnet} package with 10-fold cross validation, 
we get the $R^2 \approx 0.34$ for both Ridge and LASSO regression when predicting with \texttt{lambda.min}. 
This is not a good result, so we try the elastic net and lambda values for tuning. 
\paragraph{Elastic Net}
The elastic net is a combination of the Ridge and LASSO regression as shown in the lecture. 
The related tuning parameter is $\alpha$, which decides the proportion of the two penalties. 
In this setting, we iterate all $\alpha \in [0, 1]$ with 0.01 increments. 
In additional, we use the default lambda sequences for each $\alpha$. 
The tuning result is shown in \hyperref[fig:elastic_net]{Figure 2}.
\begin{figure}[H]
    \centering
    \includegraphics*[scale=0.25]{figures/elastic_net.png}
    \caption{Elastic Net: Tuning $\alpha$}
    \label{fig:elastic_net}
\end{figure}
It is clear that the LASSO penalty $\alpha = 1$ does not really help with the prediction.
The best $R^2$ is reached with the pure Ridge model. However, the difference is not significant, 
because of the fact that the linear assumption may not hold for this dataset.
\paragraph{Tuning Lambda}
The default lambda sequence may not be the best choice for our dataset.
Thus, we use a grid of lambda with exponential growth to find 
if a large penalty can influence the prediction. The lambda grid we use is 
\begin{align*}
    \texttt{lambda} = \texttt{exp(seq(-5, 5, 0.05))}
\end{align*}
The average tuning result with 100 simulations is shown in \hyperref[fig:lambda]{Figure 3}.
\begin{figure}[H]
    \centering
    \includegraphics*[scale=0.25]{figures/lambda.png}
    \caption{Ridge: Tuning $\lambda$}
\label{fig:lambda}
\end{figure}
Unexpectedly, the best $R^2$ is reached with the smallest lambda. 
It indicates that the Ridge penalty cannot help with the prediction, either. 
The interpretation of the bad fitting of linear regression model still lies in the absence of linear patterns in the data.
Even though Ridge penalty helps to resolve high multicollinearity and LASSO penalty 
helps to select features, they cannot help with the prediction without the linear assumption.

\subsection{Regression Model 2: *}

\subsection{Regression Model 3: LightBGM}
This model is inspired by ZhengHeng's method as presented in the literature review.
To recap the method, it 

\subsection{Classification Model: Random Forest}
For classification, we choose the random forest model using the R package \texttt{randomForest}.
The simple fitting using the default parameters gives us an \texttt{accuracy} of $\alpha = 0.2848$. 
Taking look at the confusion matrix, we found that prediction is concentrated along the diagonal, 
but does not have a high accuracy. So we present a new metric, \texttt{accuracy with tolerance}. 
This is the percentage of observations that are predicted within a tolerance $0.5$ of the true value.
\begin{align*}
    \tau = \texttt{Accuracy with tolerance} = \dfrac{\sum \texttt{diag} + \sum \texttt{subdiag} + \sum \texttt{superdiag}}{\sum \texttt{Observations}}
\end{align*}
In the simple fitting, $\tau = 0.6909$, which proves a good concentrtion of random forest model. 
\paragraph{Parameter Tuning}
The tuning goal of the model is to increase both the \texttt{accuracy} and the \texttt{accuracy with tolerance}.
Amongst many tuning parameters of random forest, we focus on \texttt{mtry} and \texttt{nodesize} with default 
\texttt{ntree} = 500. The grid we used is 
\begin{align*}
    \texttt{mtry} &\in \{1, 3, 5, 8, 10, 12, 15, 20, 25, 40,  50 \} \\
    \texttt{nodesize} &\in \{1, 3, 5, 8, 10, 12, 15, 20, 25, 40,  50 \}
\end{align*}
where the default values are $\texttt{mtry} = 3$ and $\texttt{nodesize} = 1$. 
The \hyperref[fig:contour]{contour plots} shows a best fit at $\texttt{mtry} \approx 3$ and $\texttt{nodesize} \approx 10$.
We may conclude that the effect of the random forest model is limited at $\alpha \approx 0.3$
and $\tau \approx 0.7$. Since there is an obvious concentration, 
this model is useful in the sense that it can show a general trend of the score.
However, it is not a good choice for sophisticated prediction as of the low accuracy.
\begin{figure}[H]
    \includegraphics*[scale=0.25]{figures/contour_plot_accuracy.png}
    \includegraphics*[scale=0.25]{figures/contour_plot_accuracy_with_tolerance.png}
    \caption{Contour plots of \texttt{accuracy} and \texttt{accuracy with tolerance}}
    \label{fig:contour}
\end{figure}

\paragraph{Variable Importance}
Another property of the random forest model is that it can provide a report on variable importance. 
Taking a look at \texttt{MeanDecreaseAccuracy} and \texttt{MeanDecreaseGini}, 
we found that the ratios features are much more importance than the raw value features. 
For instance, the \texttt{MeanDecreaseAccuracy} of \texttt{ratio\_word\_count} is $81.40$, 
while that of \texttt{Input\_count} is only $5.94$. We may tell from this phenomenon that
the ratio features are more informative than the raw value features. 
And a good feature engineering strategy results in better classification accuracy. 

\end{document}